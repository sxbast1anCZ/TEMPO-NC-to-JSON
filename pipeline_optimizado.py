"""
PIPELINE OPTIMIZADO v2.0 - M√°xima eficiencia tipo DATACENTER

Mejoras implementadas:
‚úÖ Procesamiento incremental (solo archivos nuevos)
‚úÖ Quality filtering inteligente con fallback autom√°tico
‚úÖ Cach√© para evitar reprocesamiento
‚úÖ Limpieza autom√°tica de archivos antiguos
‚úÖ √çndice espacial para queries r√°pidas
‚úÖ Estad√≠sticas en tiempo real
‚úÖ Manejo de errores robusto

Autor: Sebastian
Fecha: Octubre 2025
"""

import subprocess
import sys
import os
from pathlib import Path
from datetime import datetime
import json
import shutil
from quality_manager import DataQualityManager, QualityStrategy
from datacenter_optimizer import DatacenterOptimizer

def run_command(script_name, description):
    """Ejecuta un script de Python y muestra el resultado"""
    print(f"\n{'='*70}")
    print(f"> {description}")
    print(f"{'='*70}")
    
    try:
        # Configurar entorno para soportar UTF-8
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        
        result = subprocess.run(
            [sys.executable, script_name],
            capture_output=True,
            text=True,
            timeout=600,  # 10 minutos m√°ximo
            env=env,
            encoding='utf-8',
            errors='replace'
        )
        
        if result.returncode == 0:
            print(result.stdout)
            print(f"‚úÖ {description} - COMPLETADO")
            return True
        else:
            print(result.stdout)
            print(result.stderr)
            print(f"‚ùå {description} - ERROR")
            return False
            
    except subprocess.TimeoutExpired:
        print(f"‚è±Ô∏è  {description} - TIMEOUT (>10 min)")
        return False
    except Exception as e:
        print(f"‚ùå Error al ejecutar {script_name}: {e}")
        return False

def validate_data_quality():
    """Valida calidad de datos procesados"""
    print(f"\n{'='*70}")
    print("üîç Validando calidad de datos")
    print(f"{'='*70}")
    
    output_dir = Path('output')
    quality_report = {
        'files_checked': 0,
        'high_quality': 0,
        'medium_quality': 0,
        'low_quality': 0,
        'warnings': []
    }
    
    # Verificar archivos SURFACE (ya convertidos)
    for surface_file in output_dir.glob('SURFACE_*.json'):
        try:
            with open(surface_file, 'r') as f:
                data = json.load(f)
            
            manager = DataQualityManager(strategy=QualityStrategy.MODERATE)
            filtered, metadata = manager.filter_by_quality(data['measurements'])
            
            quality_report['files_checked'] += 1
            reliability = metadata['data_reliability']
            
            if reliability == "EXCELENTE" or reliability == "BUENA":
                quality_report['high_quality'] += 1
                status = "‚úÖ"
            elif reliability == "ACEPTABLE":
                quality_report['medium_quality'] += 1
                status = "‚ö†Ô∏è "
            else:
                quality_report['low_quality'] += 1
                status = "‚ùå"
                quality_report['warnings'].append(f"{surface_file.name}: {metadata.get('warning', 'Baja calidad')}")
            
            print(f"   {status} {surface_file.name}")
            print(f"      Confiabilidad: {reliability}")
            print(f"      Puntos v√°lidos: {metadata['filtered_points']:,}/{metadata['total_points']:,}")
            
        except Exception as e:
            print(f"   ‚ùå Error al validar {surface_file.name}: {e}")
    
    # Resumen
    print(f"\n{'='*70}")
    print("üìä RESUMEN DE CALIDAD")
    print(f"{'='*70}")
    print(f"   Archivos verificados: {quality_report['files_checked']}")
    print(f"   ‚úÖ Alta calidad: {quality_report['high_quality']}")
    print(f"   ‚ö†Ô∏è  Calidad media: {quality_report['medium_quality']}")
    print(f"   ‚ùå Baja calidad: {quality_report['low_quality']}")
    
    if quality_report['warnings']:
        print(f"\n‚ö†Ô∏è  ADVERTENCIAS:")
        for warning in quality_report['warnings']:
            print(f"   ‚Ä¢ {warning}")
    
    return quality_report

def clean_nc_files():
    """Elimina archivos .nc de tempo_data/ para ahorrar espacio"""
    print(f"\n{'='*70}")
    print("üßπ Limpiando archivos .nc temporales")
    print(f"{'='*70}")
    
    tempo_data = Path('tempo_data')
    nc_files = list(tempo_data.glob('*.nc'))
    
    if not nc_files:
        print("   No hay archivos .nc para eliminar")
        return
    
    total_size = 0
    for nc_file in nc_files:
        size = nc_file.stat().st_size / 1024 / 1024  # MB
        total_size += size
        print(f"   üóëÔ∏è  Eliminando: {nc_file.name} ({size:.1f} MB)")
        nc_file.unlink()
    
    print(f"\n‚úÖ Liberados: {total_size:.1f} MB")

def get_output_stats():
    """Muestra estad√≠sticas de los archivos generados"""
    print(f"\n{'='*70}")
    print("üìä ESTAD√çSTICAS DE SALIDA")
    print(f"{'='*70}")
    
    optimizer = DatacenterOptimizer()
    stats = optimizer.get_system_stats()
    
    print(f"\nüíæ Almacenamiento:")
    print(f"   Total de archivos: {stats['total_files']}")
    print(f"   Tama√±o total: {stats['total_size_gb']:.2f} GB ({stats['total_size_mb']:.1f} MB)")
    print(f"   JSONs principales: {stats['json_files']}")
    print(f"   Chunks listos para Cloud: {stats['chunk_files']}")
    
    # Detalle de archivos m√°s recientes
    output_dir = Path('output')
    json_files = sorted(output_dir.glob('SURFACE_*.json'), key=lambda x: x.stat().st_mtime, reverse=True)[:3]
    
    if json_files:
        print(f"\nüìÑ Archivos m√°s recientes:")
        for jf in json_files:
            size = jf.stat().st_size / 1024 / 1024
            modified = datetime.fromtimestamp(jf.stat().st_mtime)
            print(f"   ‚Ä¢ {jf.name}")
            print(f"     Tama√±o: {size:.1f} MB | Modificado: {modified.strftime('%Y-%m-%d %H:%M:%S')}")
            
            try:
                with open(jf, 'r') as f:
                    data = json.load(f)
                    count = len(data.get('measurements', []))
                    print(f"     Puntos: {count:,}")
            except:
                pass

def clean_old_data(days_to_keep=7):
    """Limpia datos antiguos autom√°ticamente"""
    print(f"\n{'='*70}")
    print(f"üßπ Limpieza de datos antiguos (>{days_to_keep} d√≠as)")
    print(f"{'='*70}")
    
    optimizer = DatacenterOptimizer()
    optimizer.clean_old_files(days_to_keep=days_to_keep)

def main():
    """Funci√≥n principal del pipeline optimizado"""
    start_time = datetime.now()
    
    print("\n" + "="*70)
    print("üöÄ PIPELINE OPTIMIZADO v2.0 - DATACENTER MODE")
    print("="*70)
    print(f"üìÖ Inicio: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    steps_completed = []
    steps_failed = []
    
    # PASO 1: Descargar archivos m√°s recientes
    if run_command('descargar_tempo_v2.py', 'Paso 1: Descargar archivos TEMPO'):
        steps_completed.append("Descarga")
    else:
        steps_failed.append("Descarga")
        print("\n‚ö†Ô∏è  Error en descarga. Continuando con archivos existentes...")
    
    # PASO 2: Extraer datos de archivos .nc
    if run_command('extraer_datos.py', 'Paso 2: Extraer datos de .nc'):
        steps_completed.append("Extracci√≥n")
    else:
        steps_failed.append("Extracci√≥n")
        print("\n‚ùå Error cr√≠tico en extracci√≥n. Abortando pipeline.")
        return
    
    # PASO 3: Convertir a concentraciones superficiales + AQI
    if run_command('convertir_a_superficie.py', 'Paso 3: Convertir a superficie + AQI'):
        steps_completed.append("Conversi√≥n")
    else:
        steps_failed.append("Conversi√≥n")
        print("\n‚ö†Ô∏è  Error en conversi√≥n. Continuando...")
    
    # PASO 4: Validar calidad de datos (NUEVO)
    quality_report = validate_data_quality()
    steps_completed.append("Validaci√≥n de calidad")
    
    # PASO 5: Dividir en chunks
    if run_command('dividir_archivos.py', 'Paso 5: Dividir en chunks'):
        steps_completed.append("Divisi√≥n en chunks")
    else:
        steps_failed.append("Divisi√≥n en chunks")
        print("\n‚ö†Ô∏è  Error al dividir chunks. Continuando...")
    
    # PASO 6: Limpiar archivos .nc
    clean_nc_files()
    
    # PASO 7: Limpiar datos antiguos (NUEVO)
    clean_old_data(days_to_keep=7)
    
    # PASO 8: Estad√≠sticas finales
    get_output_stats()
    
    # RESUMEN FINAL
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print(f"\n{'='*70}")
    print("üèÅ PIPELINE COMPLETADO")
    print(f"{'='*70}")
    print(f"‚è±Ô∏è  Duraci√≥n: {duration:.1f} segundos ({duration/60:.1f} minutos)")
    print(f"‚úÖ Pasos exitosos: {len(steps_completed)}")
    if steps_completed:
        for step in steps_completed:
            print(f"   ‚Ä¢ {step}")
    
    if steps_failed:
        print(f"\n‚ö†Ô∏è  Pasos fallidos: {len(steps_failed)}")
        for step in steps_failed:
            print(f"   ‚Ä¢ {step}")
    
    # Alertas de calidad
    if quality_report['low_quality'] > 0:
        print(f"\n‚ö†Ô∏è  ALERTA: {quality_report['low_quality']} archivo(s) con baja calidad")
        print("   Los datos est√°n disponibles pero marcados como baja confianza")
        print("   La API REST debe advertir a los usuarios")
    
    print(f"\nüìÖ Finalizado: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70 + "\n")
    
    # Siguiente ejecuci√≥n
    from datetime import timedelta
    next_run = end_time + timedelta(hours=3)
    print(f"‚è∞ Pr√≥xima ejecuci√≥n programada: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")
    print()

if __name__ == "__main__":
    main()
